AWSTemplateFormatVersion: '2010-09-09'
Description: 'DBT pipeline for lambda function'

Parameters:
  BucketNameapi:
    Description: Name for the bucket to store the files  
    Type: String
  layerbucketname:
    Description: Name of the bucket which has layers
    Type: String   
  requestslayername:
    Description: name of requests layer in the bucket
    Type: String
  pytzlayername:
    Description: name of pytz layer in the bucket
    Type: String
  repositoryname:
    Description: name of the repository
    Type: String
  clustername:
    Description: name of the ECS cluster
    Type: String  
  runtimename:
    Description: python runtime version
    Type: String
    Default: 'python3.12'
  Timeout:
    Description: Lambda function timeout
    Type: String
    Default: '300'
  MemorySize:
    Description: Lambda function memory in MB
    Type: String
    Default: '256'
  TaskCpu:
    Description: cpu units for task EX 256units=0.25CPU 
    Type: String
    Default: '256'
  TaskMemory:
    Description: memory units for task in MB
    Type: String
    Default: '512'
  startingdate:
    Description: start date for the scheduler in yyyy-mm-ddTHH:MM:SS.000Z
    Type: String  
    Default: '2024-03-29T00:00:00.000Z'
  EndDate: 
    Description: end date for the scheduler in yyyy-mm-ddTHH:MM:SS.000Z
    Type: String
    Default: '2025-01-01T23:59:59.000Z'
  Schedule:
    Description: schedule for the eventbridge scheduler in format 'cron(expression)'
    Type: String
    Default: 'cron(0 10 * * ? *)'
  restapiname:
    Description: name of the rest api
    Type: String
  snowflakeiamarn:
    Description: snowflake iam arn
    Type: String
  snowflakeexternalid:
    Description: snowflake external id
    Type: String

  
Resources:
  lambdaapirole:
    RoleName: lambdaapirole
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: 'Allow'
            Principal:
              Service:
                - lambda.amazonaws.com
                - states.amazonaws.com
                - events.amazonaws.com
                - scheduler.amazonaws.com
            Action: 'sts:AssumeRole'
      Policies:
        - PolicyName: lambdaapirolepolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: 'Allow'
                Action:
                  - 's3:*'
                Resource:
                  - Fn::Sub: 'arn:aws:s3:::${BucketNameapi}/*'
              - Effect: 'Allow'
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                  - 'logs:GetLogEvents'
                Resource: 'arn:aws:logs:*:*:*'
              - Effect: 'Allow'
                Action: 'sns:Publish'
                Resource: !Sub 'arn:aws:sns:us-east-1:${AWS::AccountId}:*'
              - Effect: 'Allow'
                Action: 'lambda:InvokeFunction'
                Resource: '*'
  ecsrole:
    RoleName: ecsrole
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: 'Allow'
            Principal:
              Service: ecs-tasks.amazonaws.com
            Action: 'sts:AssumeRole'
      Policies:
        - PolicyName: ecsrolepolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: 'Allow'
                Action:
                  - 'ecr:GetAuthorizationToken'
                  - 'ecr:GetDownloadUrlForLayer'
                  - 'ecr:BatchGetImage'
                Resource: '*'
              - Effect: 'Allow'
                Action:
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource:
                  - !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/ecs/MyTaskDefinition:log-stream:*'
                  - !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:${repositoryname}:*'
  ecslambdarole:
    RoleName: ecslambdarole
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: 'Allow'
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      Policies:
        - PolicyName: ecslambdarolepolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: 'Allow'
                Action: 'ecs:RunTask'
                Resource:
                  - !Sub 'arn:aws:ecs:${AWS::Region}:${AWS::AccountId}:task-definition/MyTaskDefinition:*'
              - Effect: 'Allow'
                Action: 'iam:PassRole'
                Resource: !GetAtt ecsrole.Arn
              - Effect: 'Allow'
                Action: 'ec2:DescribeSubnets'
                Resource: '*'
              - Effect: 'Allow'
                Action: 'ec2:DescribeVpcs'
                Resource: '*'
  snowflakeapigatewayrole:
    RoleName: snowflakeapigatewayrole 
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: 'Allow'
            Principal:
              AWS: !Ref snowflakeiamarn
            Action: 'sts:AssumeRole'
            Condition:
              StringLike:
                sts:ExternalId: !Ref snowflakeexternalid
      Policies:
        - PolicyName: snowflakeapigatewayrolepolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: 'Allow'
                Action: 'execute-api:Invoke'
                Resource: !Sub 'arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:*/*/POST/*'
  MyBucket:
    Type: 'AWS::S3::Bucket'
    Properties:
      BucketName: !Ref BucketNameapi
  LambdaLayerReq:
    Type: AWS::Lambda::LayerVersion
    Properties:
      LayerName: req
      Content: 
        S3Bucket: !Ref layerbucketname
        S3Key: !Ref requestslayername

  LambdaLayerPytz:
    Type: AWS::Lambda::LayerVersion
    Properties:
      LayerName: pytz1
      Content: 
        S3Bucket: !Ref layerbucketname
        S3Key: !Ref pytzlayername

  TriggerTheApi:
    Type: 'AWS::Lambda::Function'
    Properties:
      FunctionName: TriggerTheApi
      Handler: index.lambda_handler
      Role: !GetAtt lambdaapirole.Arn
      Code:
        ZipFile: |
          import boto3
          import json
          import requests
          import pytz
          import datetime
          import os

          def lambda_handler(event, context):
              # Define S3 bucket
              s3_bucket = os.environ['BUCKET_NAME']

              # Define Indian timezone
              indian_timezone = pytz.timezone('Asia/Kolkata')

              # Get current time in Indian timezone
              current_time = datetime.datetime.now(indian_timezone)
              current_timestamp = current_time.strftime('%Y-%m-%d_%H-%M-%S')

              # Define URLs for different datasets
              datasets = [
                  {"cargo": "https://data.sfgov.org/resource/u397-j8nr.json"},
                  {"passenger": "https://data.sfgov.org/resource/rkru-6vcg.json"},
                  {"landing": "https://data.sfgov.org/resource/fpux-q53t.json"}
              ]

              # Iterate over each dataset
              for dataset in datasets:
                  dataset_name, dataset_url = dataset.popitem()

                  # Initialize an empty list to store all data from the dataset
                  all_data = []

                  # Initialize pagination parameters
                  limit = 1000
                  offset = 0

                  # Fetch all data by paginating through the dataset
                  while True:
                      # Construct API URL with pagination parameters
                      api_url = f"{dataset_url}?$limit={limit}&$offset={offset}"

                      # Make API call
                      response = requests.get(api_url)

                      # Check if API call was successful
                      if response.status_code == 200:
                          data = response.json()
                          all_data.extend(data)

                          # Check if all data has been fetched
                          if len(data) < limit:
                              break

                          # Update offset for next page
                          offset += limit
                      else:
                          print(f"Failed to fetch data from {dataset_url}")
                          break

                  # Define S3 key for the file with timestamp and appropriate folder
                  s3_folder = f"{dataset_name}/"
                  s3_key = f'{s3_folder}{dataset_name}_{current_timestamp}.json'

                  # Write all data from the dataset to S3 bucket as JSON file
                  s3 = boto3.client('s3')
                  s3.put_object(Bucket=s3_bucket, Key=s3_key, Body=json.dumps(all_data))

                  # Print the number of rows in the file
                  num_rows = len(all_data)
                  print(f"Data from {dataset_name} dataset written to S3 as {s3_key}. Number of rows: {num_rows}")

      Runtime: !Ref runtimename
      Timeout: !Ref Timeout
      MemorySize: !Ref MemorySize
      Environment:
        Variables:
          BUCKET_NAME: !Ref BucketNameapi
      Layers:
        - !Ref LambdaLayerReq
        - !Ref LambdaLayerPytz

  ScheduleLambda:
    Type: 'AWS::Scheduler::Schedule'
    Properties:
      Description: This schedule invokes a Lambda function
      Name: ScheduleLambdaTrigger
      EndDate: !Ref EndDate
      FlexibleTimeWindow:
        Mode: 'OFF'
      GroupName: default
      ScheduleExpression: !Ref Schedule
      ScheduleExpressionTimezone: Asia/Kolkata
      StartDate: !Ref startingdate
      State: 'ENABLED'
      Target:
        Arn: !GetAtt TriggerTheApi.Arn
        RoleArn: !GetAtt lambdaapirole.Arn
  MyECRRepository:
    Type: 'AWS::ECR::Repository'
    Properties:
      RepositoryName: !Ref repositoryname
  MyECSCluster:
    Type: 'AWS::ECS::Cluster'
    Properties:
      ClusterName: !Ref clustername
  MyLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: testrepo
  MyTaskDefinition:
    Type: 'AWS::ECS::TaskDefinition'
    Properties:
      Family: testtaskdefinition
      Cpu: !Ref TaskCpu
      Memory: !Ref TaskMemory
      NetworkMode: awsvpc
      RequiresCompatibilities:
        - FARGATE
      ExecutionRoleArn: !GetAtt ecsrole.Arn
      TaskRoleArn: !GetAtt ecsrole.Arn
      ContainerDefinitions:
        - Name: !Ref MyLogGroup
          Image: !Sub '${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${repositoryname}:latest'
          Essential: True
          LogConfiguration:
            LogDriver: awslogs
            Options:
              awslogs-group: !Ref repositoryname
              awslogs-region: !Ref AWS::Region
              awslogs-stream-prefix: !Ref repositoryname
  MyLambdaFunctionToTriggerECS:
    Type: 'AWS::Lambda::Function'
    Properties:
      FunctionName: MyLambdaFunctionToTriggerECS
      Handler: index.lambda_handler
      Role: !GetAtt ecslambdarole.Arn
      Code:
        ZipFile: |
          import boto3
          import json
          import os

          def get_default_vpc_id():
              """
              Retrieve the ID of the default VPC associated with the AWS account.
              """
              ec2 = boto3.client('ec2')
              response = ec2.describe_vpcs(Filters=[{'Name': 'isDefault', 'Values': ['true']}])
              default_vpc = response['Vpcs'][0]
              return default_vpc['VpcId']

          def get_available_subnets(vpc_id):
              """
              Retrieve available subnets for a given VPC.
              """
              ec2_client = boto3.client('ec2')
              response = ec2_client.describe_subnets(Filters=[{'Name': 'vpc-id', 'Values': [vpc_id]}])
              return [subnet['SubnetId'] for subnet in response['Subnets']]

          def lambda_handler(event, context):
              # Initialize ECS client
              ecs_client = boto3.client('ecs')
              
              # Define ECS task parameters
              cluster_name = os.environ['clutser_name']
              task_definition = os.environ['task_definition']
              launch_type = 'FARGATE'  # or 'EC2' if you're using EC2 launch type
              
              # Retrieve VPC ID dynamically
              vpc_id = get_default_vpc_id()
              
              # Retrieve available subnets dynamically
              subnets = get_available_subnets(vpc_id)
              
              # Start ECS task
              response = ecs_client.run_task(
                  cluster=cluster_name,
                  taskDefinition=task_definition,
                  launchType=launch_type,
                  networkConfiguration={
                      'awsvpcConfiguration': {
                          'subnets': subnets,
                          'assignPublicIp': 'ENABLED'
                      }
                  }
              )

              # Check if task started successfully
              if response['failures']:
                  error_message = response['failures'][0]['reason']
                  print(f"Failed to start ECS task: {error_message}")
                  result = {
                      'statusCode': 500,
                      'body': json.dumps({
                          'data': [[0, f"Failed to start ECS task: {error_message}"]]
                      })
                  }
              else:
                  print("ECS task started successfully!")
                  result = {
                      'statusCode': 200,
                      'body': json.dumps({
                          'data': [[0, "ECS task started successfully!"]]
                      })
                  }
              
              # Return some response to indicate the Lambda function execution status
              return result

      Runtime: !Ref runtimename
      Timeout: !Ref Timeout
      MemorySize: !Ref MemorySize
      Environment:
        Variables:
          clutser_name: !Ref clustername
          task_definition: !Sub '${MyTaskDefinition}'
  MyLambdaPermission:
    Type: 'AWS::Lambda::Permission'
    Properties:
      FunctionName: !GetAtt MyLambdaFunctionToTriggerECS.Arn
      Action: 'lambda:InvokeFunction'
      Principal: 'apigateway.amazonaws.com'
      SourceArn: !Sub 'arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:*/*/POST/*'
        

  TestApi:
    Type: 'AWS::ApiGateway::RestApi'
    Properties:
      Name: !Ref restapiname
      Description: 'This is the TestApi'
      EndpointConfiguration:
        Types:
          - 'REGIONAL'
      DisableExecuteApiEndpoint: False
      ApiKeySourceType: "HEADER"
      Policy: 
        Version: "2012-10-17"
        Statement:
          - Effect: "Allow"
            Principal:
              AWS: !Sub "arn:aws:sts::${AWS::AccountId}:assumed-role/${snowflakeapigatewayrole}/snowflake"
            Action: "execute-api:Invoke"
            Resource: !Sub "arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}::*/*/*"
    DependsOn: snowflakeapigatewayrole

  Stack:
    Type: 'AWS::ApiGateway::Resource'
    Properties:
      RestApiId: !Ref TestApi
      ParentId: !GetAtt 
        - TestApi
        - RootResourceId
      PathPart: stack


  PostMethod:
    Type: 'AWS::ApiGateway::Method'
    Properties:
      RestApiId: !Ref TestApi
      ResourceId: !Ref Stack
      HttpMethod: 'POST'
      AuthorizationType: 'AWS_IAM'
      Integration:
        Type: 'AWS_PROXY'
        IntegrationHttpMethod: 'POST'
        Uri: !Sub >-
              arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${MyLambdaFunctionToTriggerECS.Arn}/invocations
        IntegrationResponses:
          - StatusCode: 200
      MethodResponses:
        - StatusCode: 200
          ResponseModels:
            application/json: 'Empty'

  Deployment:
    Type: 'AWS::ApiGateway::Deployment'
    Properties:
      RestApiId: !Ref TestApi
    DependsOn: 'PostMethod'

  ProdStage:
    Type: 'AWS::ApiGateway::Stage'
    Properties:
      StageName: 'Prod'
      Description: 'Production stage'
      RestApiId: !Ref TestApi
      DeploymentId: !Ref Deployment

